{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation\n",
    "\n",
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic segmentation using the KITTI dataset, and can be done using three methods:\n",
    "- **Full-Convolutional Network [2014]** Uses a pretrained network, upsample using deconvolution, and have skip connections to improve coarseness of upsampling.\n",
    "- **SegNet [2015]** Encoder-Decoder architecture\n",
    "- **ResNet-DUC [2017]** Dense Upsampling and Hybrid Dilated convolution\n",
    "- To improve the quality of segmentation can use something resembling FlowNet with optical flow to improve the quality of segmentation wrt. ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List all imports\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os.path as path\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline \n",
    "\n",
    "repeat=0\n",
    "import helper\n",
    "from utils import tensorflow_check,load_vggmodel,model,optimizer,training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Datasets\n",
    "root='kitti_dataset/'\n",
    "training_dataset='kitti_dataset/training'\n",
    "testing_dataset='kitti_dataset/testing'\n",
    "vgg_path='vgg/'\n",
    "save_sessions='sessions/'\n",
    "output='output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Tensorflow installation\n",
    "print(\"Tensorflow Version: \",tf.__version__)\n",
    "tensorflow_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Defining Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters and variables\n",
    "image_shape=(160,576)\n",
    "num_classes=2\n",
    "learning_rate_value=1e-4\n",
    "keep_prob_value=0.65\n",
    "n_epochs=20\n",
    "batch_size=5\n",
    "Loss_summary=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating placeholders\n",
    "label=tf.placeholder(tf.float32,[None,image_shape[0],image_shape[1],num_classes])\n",
    "learning_rate=tf.placeholder(tf.float32)\n",
    "keep_prob=tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG-16 model with skip-connections and an Adam Optimizer to minimize cross-entropy loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# def model(vgg_layer3,vgg_layer4,vgg_layer7,n_classes):\n",
    "#     # Encoder 1*1 convolutions\n",
    "#     layer_3=convolution_1x1(vgg_layer3,n_classes,'Convolution_layer3')\n",
    "#     layer_4=convolution_1x1(vgg_layer4,n_classes,'Convolution_layer4')\n",
    "#     layer_7=convolution_1x1(vgg_layer7,n_classes,'Convolution_layer7')\n",
    "    \n",
    "#     deconvolution_1=convolution_sampling(layer_7,n_classes,k_size=4,st_size=2,x_name='deconvolution_layer_1')\n",
    "#     skip_connection_1=tf.add(deconvolution_1,layer_4,name='deconvolution_layer_2')\n",
    "#     deconvolution_2=convolution_sampling(skip_connection_1,n_classes,k_size=4,st_size=2,x_name='deconvolution_layer_3')\n",
    "#     skip_connection_2=tf.add(deconvolution_2,layer_3,name='deconvolution_layer_4')\n",
    "#     output=convolution_sampling(skip_connection_2,k_size=16,st_size=8,x_name='deconvolution_output')\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def optimizer(layer,ground_truth,learning_rate,n_classes):\n",
    "    logits=tf.reshape(layer,(-1,n_classes))\n",
    "    labels=tf.reshape(ground_truth,(-1,n_classes)\n",
    "    # Cross entropy loss\n",
    "    cross_entropy_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels))\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "    train_operation=optimizer.minimizer(cross_entropy_loss)\n",
    "                     \n",
    "    return logits,cross_entropy_loss,train_operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pretrained VGG-16 model and dataset pretrained on ImageNet\n",
    "print(\"Getting images batches\")\n",
    "get_batches_fn=helper.gen_batch_function(training_dataset,image_shape)\n",
    "print(\"Running Tensorflow session\")\n",
    "with tf.Session() as session:\n",
    "    print(\"Loading pretrained VGG16 model\")\n",
    "    input_image,keep_prob,layer3,layer4,layer7=load_vggmodel(session,vgg_path)\n",
    "    print(\"Creating model with skip-layers and deconvolutions\")\n",
    "    output_layer=model(layer3,layer4,layer7,num_classes)\n",
    "    print(\"Running Adam Optimizer to minimize cross-entropy loss\")\n",
    "    logits,cross_entropy_loss,train_operation=optimizer(output_layer,label,learning_rate,num_classes)\n",
    "\n",
    "    print(\"Global Variables initializer\")\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print(\"Training\")\n",
    "    training(session,n_epochs,batch_size,get_batches_fn,train_operation,\n",
    "                 cross_entropy_loss,input_image,label,keep_prob,learning_rate,keep_prob_value,learning_rate_value)\n",
    "    print(\"Saving inference data\")\n",
    "    helper.save_inference_samples(save_sessions,root,session,image_shape,\n",
    "                                  logits,keep_prob_value,input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
